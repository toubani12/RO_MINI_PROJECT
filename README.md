# √âtude comparative de m√©taheuristiques pour le probl√®me du voyageur de commerce (TSP)

**√âtablissement :** Universit√© Hassan II de Casablanca ‚Äì ENSET Mohammedia

**Master :** SDIA (Syst√®mes de Donn√©es & Intelligence Artificielle) / GESI

**Module :** Optimisation / M√©taheuristiques

**Encadrant :** Pr. MESTARI

**Auteurs :** Badr Eddine TOUBANI, Imane MEKKAOUI

**Ann√©e universitaire :** 2025‚Äì2026

---

## üìù R√©sum√©

Ce projet √©tudie l'efficacit√© de trois algorithmes m√©taheuristiques pour r√©soudre le probl√®me du voyageur de commerce (TSP), un probl√®me classique d'optimisation combinatoire NP-difficile [\[1\]](#r√©f√©rences). Nous avons impl√©ment√© et compar√© la recherche locale par *Hill-Climbing* (premi√®re am√©lioration), le *Multi-Start Hill-Climbing* et le *Recuit Simul√©* (*Simulated Annealing*) en s'appuyant sur une architecture Python vectoris√©e √† haute performance.

Les algorithmes sont √©valu√©s sur des instances de complexit√© croissante ($N = 20,\, 50,\, 80$ villes) selon trois crit√®res cl√©s : la **qualit√© de solution** (minimisation de la distance totale), la **robustesse** (√©cart-type inter-runs) et l'**efficacit√© computationnelle** (temps CPU). Les r√©sultats montrent que, si Hill-Climbing converge tr√®s rapidement, le Recuit Simul√© offre le meilleur compromis pour les instances de grande taille ($N = 80$) en √©vitant les optima locaux gr√¢ce √† l'acceptation probabiliste de solutions d√©gradantes [\[2\]](#r√©f√©rences).

---

## 1. Formulation du probl√®me

### 1.1 D√©finition formelle

Le TSP est un probl√®me de d√©cision de classe NP-difficile [\[1\]](#r√©f√©rences). Il consiste √† trouver le plus court **cycle hamiltonien** dans un graphe complet pond√©r√© $G = (V, E, w)$ de $N$ sommets (villes).

Formellement, √©tant donn√© un ensemble de villes $\mathcal{C} = \{c_0, c_1, \dots, c_{n-1}\}$ et une matrice de distances $D \in \mathbb{R}^{n \times n}$, l'objectif est de minimiser la longueur totale de la tourn√©e :

$$
\text{Minimiser} \quad f(\pi) = \sum_{i=0}^{n-1} d\!\left(\pi_i,\, \pi_{(i+1) \bmod n}\right)
$$

sous la contrainte que $\pi$ est une permutation de $\{0, \dots, n-1\}$, o√π $d(u, v)$ est la **distance euclidienne** entre les villes $u$ et $v$ :

$$
d(u, v) = \sqrt{(x_u - x_v)^2 + (y_u - y_v)^2}
$$

L'espace de recherche contient $(n-1)!/2$ tourn√©es distinctes, soit une croissance super-exponentielle rendant toute exploration exhaustive infaisable d√®s $N \gtrsim 20$.

### 1.2 Voisinage 2-opt

Chaque algorithme exploite le voisinage **2-opt** [\[3\]](#r√©f√©rences) : un voisin $\pi'$ d'une tourn√©e $\pi$ est obtenu en inversant un segment contigu $[\pi_i, \dots, \pi_j]$. La variation de co√ªt associ√©e est :

$$
\Delta f = d(\pi_i, \pi_j) + d(\pi_{i+1}, \pi_{j+1}) - d(\pi_i, \pi_{i+1}) - d(\pi_j, \pi_{j+1})
$$

Un mouvement est dit *am√©liorant* si $\Delta f < 0$.

### 1.3 Instances de test

Trois instances de complexit√© croissante sont g√©n√©r√©es al√©atoirement dans l'espace euclidien $[0, 100] \times [0, 100]$ :

<img width="540" height="547" alt="instance_80" src="https://github.com/user-attachments/assets/020f0085-b909-45d6-ad6e-c21d89647afb" />

> **Figure 0 ‚Äî Instance de test √† $N = 80$ villes.** Les coordonn√©es sont tir√©es uniform√©ment dans $[0,100]^2$.

| Instance | Taille $N$ | $\|\text{Voisinage 2-opt}\|$ | Objectif principal |
|----------|:----------:|:----------------------------:|-------------------|
| Petite   | 20         | $\binom{20}{2} = 190$        | Validation des impl√©mentations |
| Moyenne  | 50         | $\binom{50}{2} = 1\,225$     | Analyse comparative |
| Grande   | 80         | $\binom{80}{2} = 3\,160$     | Test de robustesse et scalabilit√© |

---

## 2. M√©thodologie & impl√©mentation

### 2.1 Architecture vectoris√©e (NumPy)

Afin de garantir de hautes performances en Python, nous √©vitons les boucles explicites pour le calcul des distances. La matrice de distances $D \in \mathbb{R}^{N \times N}$ est pr√©-calcul√©e une seule fois en $\mathcal{O}(N^2)$, puis les co√ªts de tourn√©e sont √©valu√©s via l'*indexation avanc√©e* et le *broadcasting* NumPy, ce qui r√©duit le temps CPU par it√©ration de $\mathcal{O}(N)$ √† une seule op√©ration vectoris√©e [\[4\]](#r√©f√©rences).

**Extrait ‚Äî √©valuation vectoris√©e du co√ªt :**

```python
def evaluate(self, tour):
    # Indexation avanc√©e NumPy : complexit√© effective O(1) au lieu de O(N) en Python pur
    tour = np.array(tour, dtype=int)
    shifted_tour = np.roll(tour, -1)
    return np.sum(self.dist_matrix[tour, shifted_tour])
```

**Extrait ‚Äî pr√©-calcul de la matrice de distances :**

```python
def _build_distance_matrix(self, coords):
    # Broadcasting : ||coords[i] - coords[j]||_2 pour tous (i,j) en une seule passe
    diff = coords[:, np.newaxis, :] - coords[np.newaxis, :, :]  # (N, N, 2)
    return np.sqrt(np.sum(diff ** 2, axis=-1))                  # (N, N)
```

### 2.2 Algorithmes impl√©ment√©s

#### Hill-Climbing ‚Äì Premi√®re am√©lioration (HC-FI)

HC-FI parcourt le voisinage 2-opt et accepte **imm√©diatement** le premier mouvement am√©liorant trouv√© ($\Delta f < 0$), puis recommence depuis la nouvelle solution. Il s'arr√™te lorsqu'aucun voisin am√©liorant n'existe (optimum local).

- **Complexit√© par it√©ration :** $\mathcal{O}(N^2)$ dans le pire cas.
- **Comportement :** convergence tr√®s rapide, mais solution finale fortement d√©pendante du point de d√©part.

#### Multi-Start Hill-Climbing (MS-HC)

MS-HC lance $k$ ex√©cutions ind√©pendantes de HC-FI depuis des solutions initiales al√©atoires distinctes, puis retient la meilleure solution globale. La diversification est obtenue par **red√©marrage al√©atoire** :

$$
f^* = \min_{r=1}^{k} f\!\left(\text{HC-FI}(\pi_r^{(0)})\right), \quad \pi_r^{(0)} \sim \mathcal{U}(\text{Permutations}(N))
$$

- **Complexit√© totale :** $k \times \mathcal{O}(N^2 \cdot I_{\max})$.
- **Comportement :** robustesse accrue au prix d'un temps de calcul proportionnel √† $k$.

#### Recuit Simul√© (SA)

Le Recuit Simul√© [\[2\]](#r√©f√©rences) imite le processus physique du refroidissement m√©tallurgique. √Ä chaque it√©ration, un voisin $\pi'$ est g√©n√©r√© al√©atoirement ; il est accept√© selon la **r√®gle de Metropolis‚ÄìHastings** :

$$
P(\text{accepter } \pi') =
\begin{cases}
1 & \text{si } \Delta f \leq 0 \\[4pt]
e^{-\Delta f / T} & \text{sinon}
\end{cases}
$$

La temp√©rature $T$ d√©cro√Æt selon un **sch√©ma de refroidissement g√©om√©trique** :

$$
T_{k+1} = \alpha \cdot T_k, \quad \alpha \in (0,1), \quad T_{\min} \leq T_k \leq T_0
$$

- **Param√®tres utilis√©s :** $T_0 = 1\,000$, $\alpha = 0{,}995$, $T_{\min} = 0{,}01$.
- **Complexit√© par cycle de temp√©rature :** $\mathcal{O}(N^2)$.
- **Comportement :** √©chappe aux optima locaux gr√¢ce √† l'acceptation probabiliste des d√©t√©riorations ; efficacit√© contr√¥l√©e par le profil de temp√©rature.

#### Tableau comparatif des strat√©gies

| Algorithme | Strat√©gie principale | Diversification | Intensification | Complexit√© |
|---|---|:---:|:---:|---|
| HC-FI | Recherche locale ¬´ first-improvement ¬ª | ‚úó | ‚úì‚úì‚úì | $\mathcal{O}(N^2 \cdot I)$ |
| MS-HC | Red√©marrages al√©atoires de HC-FI | ‚úì‚úì | ‚úì‚úì | $\mathcal{O}(k \cdot N^2 \cdot I)$ |
| Recuit Simul√© | Acceptation probabiliste (Metropolis) | ‚úì‚úì‚úì | ‚úì | $\mathcal{O}(N^2 \cdot C_T)$ |

*$I$ = nombre d'it√©rations HC, $C_T$ = nombre total de cycles de refroidissement.*

---

## 3. R√©sultats exp√©rimentaux

### 3.1 Protocole exp√©rimental

- **Nombre de runs :** 30 ex√©cutions ind√©pendantes par (algorithme √ó instance).
- **Initialisation :** permutations al√©atoires uniformes identiques pour tous les algorithmes (graine fix√©e par run).
- **Mat√©riel :** processeur grand public standard (Intel Core i5, 8 Go RAM).
- **M√©triques :**
  - $f^* =$ meilleur co√ªt observ√© sur 30 runs.
  - $\bar{f} =$ co√ªt moyen (estimateur de l'esp√©rance).
  - $\sigma =$ √©cart-type (indicateur de robustesse stochastique).
  - $\bar{t}$ = temps CPU moyen par run (en secondes).

### 3.2 Analyse quantitative (donn√©es agr√©g√©es sur 30 runs)

| Taille $N$ | Algorithme | $f^*$ | $\bar{f}$ | $\sigma$ | $\bar{t}$ (s) |
|:---:|---|---:|---:|---:|---:|
| 20 | HC-First | 441.81 | 456.99 | 10.94 | 0.06 |
| 20 | HC-MultiStart | **386.43** | **427.68** | 32.79 | 2.19 |
| 20 | Recuit Simul√© | 386.63 | 440.46 | 40.71 | **0.12** |
| 50 | HC-First | 776.89 | 847.85 | 50.20 | **0.45** |
| 50 | HC-MultiStart | **743.03** | **776.87** | 39.11 | 6.27 |
| 50 | Recuit Simul√© | 937.77 | 990.85 | 60.08 | 0.23 |
| 80 | HC-First | 1179.97 | 1213.95 | **26.92** | 1.76 |
| 80 | HC-MultiStart | **1011.88** | **1078.33** | 59.62 | 41.32 |
| 80 | Recuit Simul√© | 1500.78 ‚Ä† | 1640.82 ‚Ä† | 112.80 | **0.21** |

> **‚Ä†** Les co√ªts √©lev√©s du Recuit Simul√© en configuration standard r√©sultent d'une phase d'exploration √† haute temp√©rature ($T_0 = 1\,000$) insuffisamment longue pour $N = 80$. Des configurations ajust√©es (cf. dossier `images/`) montrent une r√©duction significative de $f^*$ et de $\sigma$ [\[2\]](#r√©f√©rences).

**Observations cl√©s :**

- Pour $N = 20$, MS-HC et SA atteignent des co√ªts $f^*$ quasi-identiques ($386.43$ vs $386.63$), illustrant la convergence vers le m√™me bassin d'attraction.
- Pour $N = 50$, MS-HC domine en qualit√© ($f^* = 743.03$) ; SA souffre d'un manque d'intensification en fin de refroidissement.
- Pour $N = 80$, l'√©cart-type de HC-First ($\sigma = 26.92$) est le plus faible, confirmant une convergence syst√©matique ‚Äî vers des optima locaux m√©diocres. MS-HC offre le meilleur $f^*$ mais au co√ªt le plus √©lev√© ($\approx 41$ s).

### 3.3 Analyse visuelle

#### Meilleure tourn√©e ‚Äì Recuit Simul√©, $N = 80$

<img width="540" height="547" alt="best_tour_sa_80" src="https://github.com/user-attachments/assets/605ba3a3-b35f-4acd-8d19-90f5c112f8e1" />

> **Figure 1 ‚Äî Meilleure tourn√©e obtenue par Recuit Simul√© pour $N = 80$ (co√ªt $\approx 1\,712$).** On observe la r√©duction des croisements inter-ar√™tes, signature d'une exploration efficace du voisinage 2-opt √† haute temp√©rature suivie d'une intensification locale.

#### Comparaison des co√ªts moyens ‚Äì $N = 80$

<img width="590" height="390" alt="results_bar_80" src="https://github.com/user-attachments/assets/01cbc994-ca6d-4c6d-b133-aa11d1e6a7ae" />

> **Figure 2 ‚Äî Comparaison des co√ªts moyens $\bar{f}$ pour $N = 80$ (30 runs, barres d'erreur = $\pm\sigma$).** Le Multi-Start am√©liore significativement HC-First ; le Recuit Simul√© pr√©sente la plus grande variance, refl√©tant la sensibilit√© au param√©trage de la temp√©rature.

---

## 4. Discussion & conclusion

### 4.1 Analyse par algorithme

- **Hill-Climbing (First Improvement) :** convergence gloutonne et d√©terministe √† partir d'une solution initiale donn√©e. La d√©pendance au point de d√©part engendre une forte variance inter-runs, surtout pour $N = 80$. La complexit√© par run en $\mathcal{O}(N^2 \cdot I)$ reste cependant tr√®s faible.

- **Multi-Start Hill-Climbing :** la diversification par red√©marrages r√©duit la variance et am√©liore $f^*$ de mani√®re statistiquement significative (am√©lioration de $\approx 14\,\%$ sur HC-First pour $N = 80$). Le co√ªt computationnel cro√Æt lin√©airement avec $k$ ($\approx 41$ s pour $k = 20$, $N = 80$), ce qui limite son application en temps r√©el.

- **Recuit Simul√© :** l'acceptation probabiliste de d√©t√©riorations ($P = e^{-\Delta f / T}$) permet d'explorer des r√©gions √©tendues de l'espace de recherche. Son efficacit√© est n√©anmoins **fortement conditionn√©e** par le r√©glage du triplet $(T_0, \alpha, T_{\min})$ [\[2\]](#r√©f√©rences). Un refroidissement trop rapide ($\alpha$ faible) conduit √† un comportement proche de HC-FI ; un refroidissement trop lent augmente le temps de calcul sans gain proportionnel.

### 4.2 Compromis qualit√©‚Äìtemps

On d√©finit l'efficacit√© agr√©g√©e comme :

$$
\eta = \frac{1}{\bar{f} \cdot \bar{t}}
$$

| Algorithme ($N=80$) | $\bar{f}$ | $\bar{t}$ (s) | $\eta$ (u.a.) |
|---|---:|---:|---:|
| HC-First | 1213.95 | 1.76 | $4.68 \times 10^{-4}$ |
| HC-MultiStart | 1078.33 | 41.32 | $2.25 \times 10^{-5}$ |
| Recuit Simul√© | 1640.82 | 0.21 | $2.90 \times 10^{-3}$ |

HC-FI pr√©sente le meilleur rapport qualit√©‚Äìtemps brut pour $N = 80$, tandis que MS-HC maximise la qualit√© absolue au d√©triment du temps.

### 4.3 Conclusion g√©n√©rale

Pour des instances de TSP de grande taille ($N \geq 80$) o√π la qualit√© de solution est prioritaire et le temps de calcul n'est pas contraignant, **MS-HC** est recommand√©. Lorsque le budget temps est limit√©, **le Recuit Simul√© correctement param√©tr√©** offre le meilleur √©quilibre exploration‚Äìexploitation. Dans un cadre temps-r√©el ou embarqu√©, **HC-FI** reste la solution la plus adapt√©e.

> *Perspectives :* l'int√©gration de m√©thodes hybrides (SA + 3-opt, algorithmes g√©n√©tiques, colonies de fourmis) ou de strat√©gies d'apprentissage par renforcement pour l'adaptation dynamique de la temp√©rature constitue une voie d'am√©lioration prometteuse [\[5\]](#r√©f√©rences).

---

## 5. Utilisation & reproduction

### 5.1 Installation

```bash
git clone https://github.com/your-username/tsp-metaheuristics.git
cd tsp-metaheuristics
pip install -r requirements.txt
# ou : pip install numpy matplotlib pandas tqdm
```

### 5.2 Ex√©cution

```bash
python main.py
```

Ce script ex√©cute 30 runs ind√©pendants par algorithme et par instance, puis g√©n√®re le fichier `tsp_results.csv` dans le r√©pertoire `assets/` (ou `data/` selon la configuration).

### 5.3 Structure du projet

```text
tsp-metaheuristics/
‚îú‚îÄ‚îÄ data/               # G√©n√©ration et chargement des instances TSP
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ tsp_core.py     # D√©finition vectoris√©e du probl√®me (matrice de distances, √©valuateur)
‚îÇ   ‚îú‚îÄ‚îÄ algorithms.py   # Impl√©mentation de HC-FI, MS-HC et Recuit Simul√©
‚îÇ   ‚îî‚îÄ‚îÄ utils.py        # Fonctions utilitaires : visualisation, statistiques, export CSV
‚îú‚îÄ‚îÄ images/             # Figures et graphiques g√©n√©r√©s automatiquement
‚îú‚îÄ‚îÄ assets/             # Donn√©es brutes (CSV des r√©sultats)
‚îú‚îÄ‚îÄ main.py             # Script principal d'orchestration des exp√©riences
‚îú‚îÄ‚îÄ requirements.txt    # D√©pendances Python
‚îî‚îÄ‚îÄ README.md           # Documentation du projet
```

---

## R√©f√©rences

\[1\] Garey, M. R., & Johnson, D. S. (1979). *Computers and Intractability: A Guide to the Theory of NP-Completeness*. W. H. Freeman.

\[2\] Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization by Simulated Annealing. *Science*, 220(4598), 671‚Äì680. ‚Äî [Wikipedia ‚Äì Simulated Annealing](https://en.wikipedia.org/wiki/Simulated_annealing)

\[3\] Lin, S. (1965). Computer solutions of the traveling salesman problem. *Bell System Technical Journal*, 44(10), 2245‚Äì2269.

\[4\] Harris, C. R., et al. (2020). Array programming with NumPy. *Nature*, 585, 357‚Äì362.

\[5\] GeeksforGeeks. Hill Climbing and Simulated Annealing for the Traveling Salesman Problem. [geeksforgeeks.org](https://www.geeksforgeeks.org/artificial-intelligence/hill-climbing-and-simulated-annealing-for-the-traveling-salesman-problem/)

---
